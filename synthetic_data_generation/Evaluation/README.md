# Synthetic Data Quality Evaluation

This folder details our pipeline for measuring the quality of the synthetic datasets generated after running `run_data_gen_pipeline.py`. Utilities are contained and detailed in `evaluate.py`. The main script is `generate_evals.py`.


## Evaluation procedure

We use the Python library sdmetrics, also used in Shi et al. (2025) to evaluate each experimental run. For each such run, we compute four scores:
- Column shape similarity score: for each variable $X$, the distributions of $X$ in the real and synthetic data are compared using the Kolmogorov-Smirnov statistic or Total Variation Distance, depending on whether $X$ is continuous or discrete. A score of 1 indicates a perfect match between the real and synthetic columns, while a score of 0 indicates no relationship between them.
- Column pair trends similarity score: for each pair of variables $(X, Y)$, the joint distribution of $(X, Y)$ in the real and synthetic data are compared based on the Total Variation Distance. As before, a score of 1 indicates a perfect match between the trends in the real and synthetic columns, while a score of 0 indicates no relationship.
- Classifier 2-sample test score (C2ST): computes a metric that indicates how difficult it is to tell apart the synthetic data from the real data. A score of 1 indicates that the model does not distinguish at all between the real and synthetic rows (i.e., does not do better than chance), while a score of 0 indicates that it perfectly distinguishes the real from the synthetic data. That is, we seek a higher score so that our synthetic data matches the real data as most as possible.
- Overall similarity score: the average of the column shape and column pair trends score. As before, a score of 1 indicates a perfect match between the trends in the real and synthetic columns, while a score of 0 indicates no relationship.


## Requirements

The following libraries are required:
- sdmetrics: routines for synthetic data evaluation.
- Plotly: manipulating output visualizations from sdmetrics.
- Pillow: manipulating images.
- tqdm: progress bar.
- Pandas: tabular data manipulation.

To run `generate_evals.py`, the following is required:

- Original data files for each wave. generate_evals.py assumes these are structured within the same folder (in this project, the "Original data" folder).

- Synthetic data files, for each model and each wave. generate_evals.py assumes these are structured within the same folder (in this project, the "Final_gen_data" folder), and that each of those folders contains a subfolder with the synthetic data for each model. Sample file structure:

```
Final_gen_data
├── claude-sonnet-4
│   └── gen_data_wave1.csv
│   └── gen_data_wave2.csv
│   └── gen_data_wave3.csv
│     
└── gpt-5
    └── gen_data_wave1.csv
    └── gen_data_wave2.csv
    └── gen_data_wave3.csv
```

- Evaluation metadata JSON files for each wave. See the README file in the folder `./Evaluation metadata`.


## Output

Output scores are stored in `./Evaluation metadata`. See the README file in such folder to examine the output structure.

This folder contains the evaluation metrics for all models across all waves generated by `generate_evals.py`. Output scores are stored in `./Evaluation metadata`. See the README file in such folder to examine the output structure.

As an optional functionality, a plot comparing the distributions of all variables in the real and synthetic datasets for each model and each wave can be generated. This uses the `get_column_plot` function from sdmetrics, and the Image module to paste plots together. This plot is stored in a directory for each model. A sample output file structure would be the following:

```
Evaluation
├── claude-sonnet-4 (Optional folder)
│   └── claude-sonnet-4_wave1_results.png
│   └── claude-sonnet-4_wave2_results.png
│
├── gpt-5 (Optional folder)
│   └── gpt-5_wave1_results.png
│   └── gpt-5_wave2_results.png
│
├── Evaluation metadata
│   └── wave1_eval_meta.json
│   └── wave2_eval_meta.json
│
├── Evaluation results
│   └── c2st.csv
│   └── density_overall.csv
│   └── density_shape.csv
│   └── density_trend.csv
│
├── evaluate.py
├── generate_evals.py
└── README.md

    
```

The code can be found in `generate_evals.py` and `evaluate.py`.



## Further references

See the sdmetrics documentation for more information on the evaluation methods. Also see

Shi, J.; Xu, M.; Hua, H.; Zhang, H.; Ermon, S.; and Leskovec, J. 2025. TabDiff: a Mixed-type Diffusion Model for Tabular Data Generation. arXiv:2410.20626.